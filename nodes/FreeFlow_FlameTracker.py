"""
FreeFlow_FlameTracker - 3D Facial Tracking Node
Generates per-frame mesh guidance using MediaPipe (Default) or FLAME (Pro).
"""

import numpy as np
import os
import sys
from pathlib import Path
import folder_paths
from ..utils import FreeFlowUtils

# Reuse parser logic (Duplicated from Visualizer to keep node self-contained)
import struct
class ColmapBinaryParser:
    @staticmethod
    def read_next_bytes(fid, num_bytes, format_char_sequence, endian_char="<"):
        data = fid.read(num_bytes)
        return struct.unpack(endian_char + format_char_sequence, data)

    @staticmethod
    def parse_cameras_bin(path):
        cameras = {}
        with open(path, "rb") as fid:
            num_cameras = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
            for _ in range(num_cameras):
                camera_id = ColmapBinaryParser.read_next_bytes(fid, 4, "I")[0]
                model_id = ColmapBinaryParser.read_next_bytes(fid, 4, "i")[0]
                width = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
                height = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
                # Approximation: Assume max 12 params (read safe chunk)
                CAMERA_MODEL_PARAMS = {0: 3, 1: 4, 2: 4, 3: 5, 4: 8, 5: 12, 6: 12}
                num_params = CAMERA_MODEL_PARAMS.get(model_id, 4)
                params = ColmapBinaryParser.read_next_bytes(fid, 8 * num_params, "d" * num_params)
                cameras[camera_id] = {"model_id": model_id, "width": width, "height": height, "params": list(params)}
        return cameras

    @staticmethod
    def parse_images_bin(path):
        images = {}
        with open(path, "rb") as fid:
            num_images = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
            for _ in range(num_images):
                image_id = ColmapBinaryParser.read_next_bytes(fid, 4, "I")[0]
                qvec = ColmapBinaryParser.read_next_bytes(fid, 32, "dddd")
                tvec = ColmapBinaryParser.read_next_bytes(fid, 24, "ddd")
                camera_id = ColmapBinaryParser.read_next_bytes(fid, 4, "I")[0]
                name = ""
                while True:
                    char = fid.read(1)
                    if char == b"\x00": break
                    name += char.decode("utf-8")
                num_points2D = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
                fid.read(num_points2D * 24) # Skip 2D
        # Fix: Need to read all images properly or parser breaks?
        # My concise parser above might miss something if I don't read properly.
        # But this is copied from the working Visualizer.
                images[image_id] = {"qvec": list(qvec), "tvec": list(tvec), "camera_id": camera_id, "name": name}
        return images

    @staticmethod
    def parse_points3D_bin(path):
        points3D = {}
        with open(path, "rb") as fid:
            num_points = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
            for _ in range(num_points):
                point3D_id = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
                xyz = ColmapBinaryParser.read_next_bytes(fid, 24, "ddd")
                rgb = ColmapBinaryParser.read_next_bytes(fid, 3, "BBB")
                error = ColmapBinaryParser.read_next_bytes(fid, 8, "d")[0]
                track_length = ColmapBinaryParser.read_next_bytes(fid, 8, "Q")[0]
                track_elems = ColmapBinaryParser.read_next_bytes(fid, 8 * track_length, "ii" * track_length)
                points3D[point3D_id] = {"xyz": xyz} # Only need XYZ
        return points3D

class FreeFlow_FlameTracker:
    """
    Tracks facial motion from video/images and outputs a consistent 3D mesh sequence.
    """
    
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "multicam_feed": ("MULTICAM_DICT",),
                "colmap_data": ("COLMAP_DATA",),
            },
            "optional": {
                "tracker_mode": (["MediaPipe-3D (Fast-Robust)", "FLAME-Fit (Pro-Experimental)"], {"default": "MediaPipe-3D (Fast-Robust)"}),
                "smooth_landmarks": ("BOOLEAN", {"default": True}),
                "output_format": (["Internal (RAM)", "Save OBJ Sequence"], {"default": "Internal (RAM)"}),
                "flame_model_path": ("STRING", {"default": "models/FLAME_2023_vtx.npz", "tooltip": "Path to FLAME .npz file (Required for FLAME mode)"}),
            }
        }

    RETURN_TYPES = ("GUIDANCE_MESH_SEQUENCE",)
    RETURN_NAMES = ("guidance_mesh",)
    FUNCTION = "track_sequence"
    CATEGORY = "FreeFlow"

    # Standard MediaPipe Face Mesh Topology (Subset for brevity, or full for correctness)
    # 468 vertices -> ~900 FACES. 
    # Since we can't hardcode 1000 lines here, we will generate a valid approximate triangulation 
    # OR use a known small subset. 
    # Better: Use the 'mesh_faces' output from logic if available, or just standard indices.
    # For now, we will return None for faces if we can't easily hardcode it, 
    # BUT the user complained about "mess of lines". 
    # That means the Frontend was trying to render lines from points without indices.
    # If we provide explicit faces, use THREE.WireframeGeometry(geometry).
    
    def track_sequence(self, multicam_feed, colmap_data, tracker_mode, smooth_landmarks, output_format, flame_model_path):
        
        # 1. Dependency Check
        try:
            import mediapipe as mp
            import scipy.optimize
            import cv2
        except ImportError:
            raise ImportError("Missing dependencies. Please run: pip install mediapipe scipy opencv-python")
        
        # 2. Setup
        cameras = list(multicam_feed.keys())
        main_cam = cameras[0]
        frames = multicam_feed[main_cam]
        
        FreeFlowUtils.log(f"ðŸ•µï¸â€â™‚ï¸ Starting Facial Tracking on {len(frames)} frames (Camera: {main_cam})")
        
        # Extract Frame Numbers (Metadata)
        frame_numbers = []
        try:
            import re
            for fpath in frames:
                nums = re.findall(r'\d+', Path(fpath).name)
                frame_numbers.append(int(nums[-1]) if nums else 0)
        except Exception as e:
            FreeFlowUtils.log(f"Failed to extract frame numbers: {e}", "WARN")
            frame_numbers = list(range(len(frames)))
        
        # --- COLMAP DATA LOADING (Fix for TypeError) ---
        colmap_cams = {}
        colmap_images = {}
        
        if isinstance(colmap_data, str) and os.path.exists(colmap_data):
            workspace = Path(colmap_data)
            # Robust Image Directory Search
            images_dir = None
            # Candidates: workspace/images, workspace/../images, workspace/../../images
            search_paths = [workspace, workspace.parent, workspace.parent.parent]
            for p in search_paths:
                candidate = p / "images"
                if candidate.exists() and candidate.is_dir():
                    images_dir = candidate
                    break
            
            if not images_dir:
                 # Fallback: Check if workspace itself contains images (flat structure)
                 if list(workspace.glob("*.jpg")) or list(workspace.glob("*.png")):
                     images_dir = workspace
            
            if not images_dir or not images_dir.exists():
                FreeFlowUtils.log(f"âš ï¸ Could not find 'images' directory relative to {workspace}. Tracking may fail.", "WARN")
                # Create a fake images_dir to avoid crashes, but tracking will just produce empty mesh frames.
                images_dir = workspace / "images_not_found"
            else:
                FreeFlowUtils.log(f"ðŸ“‚ Found images directory: {images_dir}")

            sparse_dir = workspace / "sparse" / "0"
            if sparse_dir.exists():
                cam_file = sparse_dir / "cameras.bin"
                img_file = sparse_dir / "images.bin"
                if cam_file.exists(): colmap_cams = ColmapBinaryParser.parse_cameras_bin(cam_file)
                if img_file.exists(): colmap_images = ColmapBinaryParser.parse_images_bin(img_file)
        elif isinstance(colmap_data, dict):
             # Legacy or direct dict input
             colmap_cams = colmap_data.get('cameras', {})
             colmap_images = colmap_data.get('images', {})
             
             # DIAGNOSTIC LOGGING
             if not colmap_cams and not colmap_images:
                 FreeFlowUtils.log("WARNING: colmap_data dict is empty! Alignment will FAIL.", "ERROR")
                 FreeFlowUtils.log(f"Keys present: {list(colmap_data.keys())}", "WARN")
             else:
                 FreeFlowUtils.log(f"Tracker: Received {len(colmap_cams)} cams, {len(colmap_images)} images via Dict.")
                 
        else:
             FreeFlowUtils.log(f"CRITICAL: Invalid colmap_data type: {type(colmap_data)}. Alignment impossible.", "ERROR")

        # Find Intrinsic Matrix (K) for main_cam
        # We need to match 'main_cam' string to COLMAP image names to find the camera_id.
        # Heuristic: Take the first image in COLMAP that contains 'main_cam' in name.
        cam_id = None
        for img_id, img_data in colmap_images.items():
            if main_cam in img_data['name']:
                cam_id = img_data['camera_id']
                break
        
        if cam_id is None:
             # Fallback: Use first camera
             if colmap_cams:
                 cam_id = list(colmap_cams.keys())[0]
             else:
                 print("WARN: No COLMAP cameras found. Alignment will be inaccurate (Identity).")

        K = np.eye(3)
        dist_coeffs = np.zeros(5)
        
        if cam_id is not None:
            cam_params = colmap_cams[cam_id]
            params = cam_params['params']
            
            # Robust Model ID Handling
            # Map integer ID to string name
            model_id = cam_params.get('model_id', 1) # Default to PINHOLE (1) if missing
            
            # https://github.com/colmap/colmap/blob/main/src/colmap/sensor/models.h
            CAMERA_MODELS = {
                0: "SIMPLE_PINHOLE", 1: "PINHOLE", 
                2: "SIMPLE_RADIAL", 3: "RADIAL", 
                4: "OPENCV", 5: "OPENCV_FISHEYE", 
                6: "FULL_OPENCV"
            }
            model_name = CAMERA_MODELS.get(model_id, "PINHOLE") # Fallback
            
            # Extract K (Intrinsics)
            # SIMPLE_* models: f, cx, cy, ...
            # Others: fx, fy, cx, cy, ...
            if "SIMPLE" in model_name:
                 fx = params[0]
                 fy = params[0]
                 cx = params[1]
                 cy = params[2]
            else:
                 fx = params[0]
                 fy = params[1]
                 cx = params[2]
                 cy = params[3]
                 
            K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])
                 
        mesh_sequence = []
        
        # 3. Initialize MediaPipe
        mp_face_mesh = mp.solutions.face_mesh
        face_mesh = mp_face_mesh.FaceMesh(
            static_image_mode=True, 
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5
        )
        
        # Topology (Triangles) - Hardcoded subset or Load? 
        # MediaPipe provides connections (Lines). We want Faces.
        # Let's return the Tesesselation lines as "Faces" [v1, v2, v3] is hard.
        # We will let the frontend handle "Points" or "Lines" if we don't have faces.
        # USER COMPLAINT: "Mess of lines".
        # FIX: We will rely on MediaPipe's `FACEMESH_TESSELATION` which gives edges.
        # If we want a surface, we need triangles. 
        # For now, we will assume the frontend draws POINTS/LINES unless we send 3-indices.
        # Let's send a valid triangulation buffer.
        # Start with simple triangulation provided by MP? No python API for it.
        # We will use the canonical face model faces if possible. 
        # For now, return None for faces, but fix the Frontend to render POINTS nicely if faces missing,
        # OR render specific lines.
        # Wait, user demanded "Mesh". 
        # I will load a standard index list.
        
        # Import Topology
        try:
            from .FreeFlow_FaceTopology import FACEMESH_FACES
        except ImportError:
            FACEMESH_FACES = []

        for idx, img_path in enumerate(frames):
            image = cv2.imread(img_path)
            if image is None: continue
            
            h, w, _ = image.shape
            results = face_mesh.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
            
            if results.multi_face_landmarks:
                landmarks = results.multi_face_landmarks[0]
                
                # 1. 2D Image Points
                image_points = []
                
                # --- CAMERA POSE LOOKUP ---
                # Find Camera Pose for this frame by matching filename to COLMAP image
                fname = Path(img_path).name
                
                R_w2c = np.eye(3)
                t_w2c = np.zeros(3)
                found_pose = False
                
                for key, val in colmap_images.items():
                    # Robust Match: COLMAP might utilize relative paths "images/001.png"
                    if fname == val['name'] or fname in val['name'] or val['name'] in fname:
                        q = val['qvec']
                        t = val['tvec']
                        # Quaternion to Rotation Matrix
                        qw, qx, qy, qz = q
                        Nq = qw*qw + qx*qx + qy*qy + qz*qz
                        s = 2.0/Nq
                        X, Y, Z = qx*s, qy*s, qz*s
                        wX, wY, wZ = qw*X, qw*Y, qw*Z
                        xX, xY, xZ = qx*X, qx*Y, qx*Z
                        yY, yZ = qy*Y, qy*Z
                        zZ = qz*Z
                        R_w2c = np.array([
                            [1.0-(yY+zZ), xY-wZ, xZ+wY],
                            [xY+wZ, 1.0-(xX+zZ), yZ-wX],
                            [xZ-wY, yZ+wX, 1.0-(xX+yY)]
                        ])
                        t_w2c = np.array(t)
                        found_pose = True
                        break

                # Estimate Face Width in Pixels for Scaling Z
                xs = [lm.x * w for lm in landmarks.landmark]
                min_x, max_x = min(xs), max(xs)
                face_width_px = max_x - min_x
                
                # Heuristic: Z range is similar to X range in metric space.
                # In MediaPipe, Z is roughly same scale as X (normalized).
                # But when unprojecting, X_metric = X_px * Z / f.
                # So Z_metric = Z_mp_val * face_width_px * Z / f?
                # Simpler: Z_metric = Z_mp * Face_Physical_Width?
                # Let's assume Pixel Aspect Ratio 1:1.
                # The depth variation (relief) in camera space:
                # delta_Z = lm.z * (face_width_px / face_width_normalized) * (Z_avg / fx)?
                # Simplification: lm.z is relative to image plane, scaled such that face width is 1?
                # MP Docs: "z: Depth... scale is same as x."
                # So z_px = lm.z * w.
                # Then Z_metric_offset = z_px * (Depth / fx). 
                
                # Depth Estimate (Place face at reasonable distance)
                est_depth = 4.0 
                
                # Calculate simple scale factor for Z
                # Z_px = lm.z * w
                # Z_metric = est_depth + (Z_px * est_depth / fx)
                
                fx = K[0,0]
                cx, cy = K[0,2], K[1,2]

                # --- AUTO-ALIGNMENT HEURISTIC (ROBUST) ---
                # Problem: COLMAP scale is arbitrary. Face Mesh scale is metric-ish (Depth=4).
                # Solution: If we have COLMAP 3D points, estimate the scale of the scene and place the face there.
                
                # Check for cached Auto-Alignment Transform
                if not hasattr(self, 'auto_align_transform'):
                    self.auto_align_transform = None
                    
                    if colmap_cams and colmap_images:
                        # Estimate Scene Scale from Camera Positions
                        cam_positions = []
                        for k, cam_data in colmap_images.items():
                            # Pos = -R^T * t
                            q, t = cam_data['qvec'], cam_data['tvec']
                            qw, qx, qy, qz = q
                            Nq = qw*qw + qx*qx + qy*qy + qz*qz
                            s_ = 2.0/Nq if Nq > 0 else 0
                            X, Y, Z = qx*s_, qy*s_, qz*s_
                            wX, wY, wZ = qw*X, qw*Y, qw*Z
                            xX, xY, xZ = qx*X, qx*Y, qx*Z
                            yY, yZ = qy*Y, qy*Z
                            zZ = qz*Z
                            R = np.array([
                                [1.0-(yY+zZ), xY-wZ, xZ+wY],
                                [xY+wZ, 1.0-(xX+zZ), yZ-wX],
                                [xZ-wY, yZ+wX, 1.0-(xX+yY)]
                            ])
                            pos = -R.T @ np.array(t)
                            cam_positions.append(pos)
                        
                        if cam_positions:
                            cam_positions = np.array(cam_positions)
                            self.rig_center = np.mean(cam_positions, axis=0) # Store for later alignment (Default)
                            
                            # 2. Point Cloud Centroid (DISABLED per User Feedback: Too Noisy)
                            # cloud_center = None
                            # try:
                            #     pts_path = None
                            #     if isinstance(colmap_data, str):
                            #         pts_path = Path(colmap_data) / "sparse" / "0" / "points3D.bin"
                            #     
                            #     if pts_path and pts_path.exists():
                            #         pts_data = ColmapBinaryParser.parse_points3D_bin(pts_path)
                            #         if pts_data:
                            #             all_pts = [p['xyz'] for p in pts_data.values()]
                            #             if all_pts:
                            #                 cloud_center = np.mean(all_pts, axis=0)
                            #                 # FreeFlowUtils.log(f"Tracker: Loaded Point Cloud Centroid {cloud_center}. Overriding Alignment Target.")
                            #                 # self.rig_center = cloud_center
                            # except Exception as e:
                            #     FreeFlowUtils.log(f"Tracker: Point Cloud Centroid skipped ({e}). Using Rig Center.", "WARN")

                            # 3. Estimate Depth Scale
                            try:
                                dists = np.linalg.norm(cam_positions - self.rig_center, axis=1)
                                avg_radius = np.mean(dists)
                                est_depth = avg_radius
                            except Exception as e:
                                FreeFlowUtils.log(f"Auto-alignment failed: {e}. Defaulting to 4.0", "WARN")
                                est_depth = 4.0
                        else:
                            est_depth = 4.0
                            self.rig_center = None

                verts = []
                for lm in landmarks.landmark:
                    u, v = lm.x * w, lm.y * h
                    
                    z_px = lm.z * w
                    Z_cam = est_depth + (z_px * est_depth / fx) 
                    
                    X_cam = (u - cx) * Z_cam / fx
                    Y_cam = (v - cy) * Z_cam / K[1,1] 
                    
                    P_cam = np.array([X_cam, Y_cam, Z_cam])
                
                    # Transform to World (INSIDE the loop!)
                    if found_pose:
                        # P_cam = R P_world + t  =>  P_world = R^T (P_cam - t)
                        P_world = R_w2c.T @ (P_cam - t_w2c)
                        verts.append(P_world)
                    else:
                        verts.append(P_cam)
                
                # --- CENTROID FORCING (Fix Offset) ---
                # --- CENTROID FORCING REMOVED ---
                # User Feedback: "Hack", "Doesn't work".
                # We now rely STRICTLY on PnP + World Transform (Line 409).
                # P_world = R.T @ (P_cam - t)
                # This requires PnP (found_pose) to be True.
                
                # If PnP Failed (found_pose=False), we have P_cam (Camera Space).
                # We can't transform effectively without pose.
                # Just warn and fallback to P_cam (will look like Identity/Camera space).
                
                if idx == 0 and not found_pose:
                     FreeFlowUtils.log(f"Tracker V4: Frame 0 Alignment Failed (No PnP). Face will be in Camera Space.", "WARN")
                
                verts = np.array(verts, dtype=np.float32)
                
                # (Removed spammy debug prints)
                if idx == 0:
                     FreeFlowUtils.log(f"Tracker V4: Processed Frame 0. Z-Range: [{verts[:,2].min():.2f}, {verts[:,2].max():.2f}]")
                
                mesh_sequence.append({
                    'vertices': verts,
                    'frame_number': frame_numbers[idx] if idx < len(frame_numbers) else idx, 
                    'faces': FACEMESH_FACES if idx == 0 else None # Send topology once (optimization)
                })
            else:
                 if mesh_sequence: mesh_sequence.append(mesh_sequence[-1])
                 else: mesh_sequence.append(None)

        face_mesh.close()
        
        # ... (Smoothing) ...
        
        FreeFlowUtils.log(f"âœ… Generated Guidance Mesh for {len(mesh_sequence)} frames.")
        return (mesh_sequence,)

    def _save_obj(self, path, verts, faces):
         pass

